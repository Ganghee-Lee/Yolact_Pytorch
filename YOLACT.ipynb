{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YOLACT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOjpQryvgiIV8A/g6a49AEz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kanghee-Lee/Yolact_Pytorch/blob/master/YOLACT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdA7OjafW15A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "backbone_selected_layers=[1, 2, 3]\n",
        "\n",
        "\n",
        "############### Backbone ###############\n",
        "class Bottleneck(nn.Module):\n",
        "    '''\n",
        "    Bottleneck : Convolutional Block & Identity Block\n",
        "    Identity Block's downsample is None\n",
        "    Convolutional Block's downsample consists of conv2D and BatchNorm\n",
        "\n",
        "    Bottleneck is implemented as ResNet. \n",
        "    You can see ResNet's structure and parameters in https://ganghee-lee.tistory.com/41?category=863370\n",
        "    '''\n",
        "    expansion = 4\n",
        "\n",
        "    # Only conv2 kernel size is 3, others are 1\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, norm_layer=nn.BatchNorm2d):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = norm_layer(planes * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bG0P4RqxgY5J",
        "colab_type": "code",
        "outputId": "081320a3-c141-4ea2-b396-040183a8c8b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "class ResNetBackbone(nn.Module):\n",
        "    '''\n",
        "    Implemented using Bottleneck.\n",
        "    '''\n",
        "    def __init__(self, layers, block=Bottleneck, norm_layer=nn.BatchNorm2d):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_base_layers = len(layers)\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.channels = []\n",
        "        self.norm_layer = norm_layer\n",
        "        self.inplanes = 64\n",
        "        # stage 1 : (3, 550, 550) -> (64, 138, 138)\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        # conv1's stride is 2, it reduces input size by 2.\n",
        "        self.bn1 = norm_layer(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        # maxpool reduces input size by 2. Finally stage 1 shrinks size by 4.\n",
        "\n",
        "        self.make_layer(block, 64, layers[0])              # stage 2\n",
        "        self.make_layer(block, 128, layers[1], stride=2)   # stage 3\n",
        "        self.make_layer(block, 256, layers[2], stride=2)   # stage 4\n",
        "        self.make_layer(block, 512, layers[3], stride=2)   # stage 5\n",
        "\n",
        "        self.backbone_modules = [m for m in self.modules() if isinstance(m, nn.Conv2d)]\n",
        "\n",
        "    def make_layer(self, block, planes, blocks, needsConv=True, stride=1):\n",
        "        downsample = None\n",
        "\n",
        "        # All stages (except stage 1) has conv block, and it reduces img size by half .(stage 2 has conv block which is stride 1)\n",
        "        # So it also does not reduces size by half.\n",
        "        # downsample consists of conv2D and Batch Norm used for Convolutional Block.\n",
        "        if needsConv:\n",
        "            downsample = nn.Sequential(nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                                                 kernel_size=1, stride=stride, bias=False),\n",
        "                                       self.norm_layer(planes * block.expansion))\n",
        "        # add convolution Block.\n",
        "        layers = [block(self.inplanes, planes, stride, downsample, self.norm_layer)]\n",
        "        self.inplanes = planes * block.expansion\n",
        "\n",
        "        # add Identity Block.\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, norm_layer=self.norm_layer))\n",
        "\n",
        "        layer = nn.Sequential(*layers)\n",
        "\n",
        "        self.channels.append(planes * block.expansion)\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" Returns a list of convouts for each layer. \"\"\"\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        outs = []\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = layer(x)\n",
        "            outs.append(x)\n",
        "\n",
        "        return tuple(outs)\n",
        "# --------------------------------------------------------------------------------------------------------------------------------\n",
        "    def init_backbone(self, path):\n",
        "        \"\"\" Initializes the backbone weights for training. \"\"\"\n",
        "        state_dict = torch.load(path)\n",
        "\n",
        "        # Replace layer1 -> layers.0 etc.\n",
        "        keys = list(state_dict)\n",
        "        for key in keys:\n",
        "            if key.startswith('layer'):\n",
        "                idx = int(key[5])\n",
        "                new_key = 'layers.' + str(idx - 1) + key[6:]\n",
        "                state_dict[new_key] = state_dict.pop(key)\n",
        "\n",
        "        # Note: Using strict=False is berry scary. Triple check this.\n",
        "        self.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "    def add_layer(self, conv_channels=1024, downsample=2, depth=1, block=Bottleneck):\n",
        "        \"\"\" Add a downsample layer to the backbone as per what SSD does. \"\"\"\n",
        "        self.make_layer(block, conv_channels // block.expansion, blocks=depth, stride=downsample)\n",
        "\n",
        "def construct_backbone(cfg_backbone=ResNetBackbone):\n",
        "    # resnet101 has 3, 4, 23, 3 blocks for each stage\n",
        "    backbone = cfg_backbone([3, 4, 23, 3])\n",
        "\n",
        "    # Add downsampling layers until we reach the number we need\n",
        "    selected_layers=[1, 2, 3]\n",
        "    num_layers = max(selected_layers) + 1\n",
        "\n",
        "    while len(backbone.layers) < num_layers:\n",
        "        backbone.add_layer()\n",
        "\n",
        "    return backbone\n",
        "\n",
        "from torchvision import models\n",
        "from torchsummary import summary\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
        "model = construct_backbone().to(device)\n",
        "summary(model, (3, 550, 550))\n",
        "input=torch.randn(1, 3, 550, 550)\n",
        "backbone=construct_backbone()(input)\n",
        "\n",
        "print('backbone output feature 개수 :', len(backbone))\n",
        "print('C2 output shape : ', backbone[0].shape)\n",
        "print('C3 output shape : ', backbone[1].shape)\n",
        "print('C4 output shape : ', backbone[2].shape)\n",
        "print('C5 output shape : ', backbone[3].shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 275, 275]           9,408\n",
            "       BatchNorm2d-2         [-1, 64, 275, 275]             128\n",
            "              ReLU-3         [-1, 64, 275, 275]               0\n",
            "         MaxPool2d-4         [-1, 64, 138, 138]               0\n",
            "            Conv2d-5         [-1, 64, 138, 138]           4,096\n",
            "       BatchNorm2d-6         [-1, 64, 138, 138]             128\n",
            "              ReLU-7         [-1, 64, 138, 138]               0\n",
            "            Conv2d-8         [-1, 64, 138, 138]          36,864\n",
            "       BatchNorm2d-9         [-1, 64, 138, 138]             128\n",
            "             ReLU-10         [-1, 64, 138, 138]               0\n",
            "           Conv2d-11        [-1, 256, 138, 138]          16,384\n",
            "      BatchNorm2d-12        [-1, 256, 138, 138]             512\n",
            "           Conv2d-13        [-1, 256, 138, 138]          16,384\n",
            "      BatchNorm2d-14        [-1, 256, 138, 138]             512\n",
            "             ReLU-15        [-1, 256, 138, 138]               0\n",
            "       Bottleneck-16        [-1, 256, 138, 138]               0\n",
            "           Conv2d-17         [-1, 64, 138, 138]          16,384\n",
            "      BatchNorm2d-18         [-1, 64, 138, 138]             128\n",
            "             ReLU-19         [-1, 64, 138, 138]               0\n",
            "           Conv2d-20         [-1, 64, 138, 138]          36,864\n",
            "      BatchNorm2d-21         [-1, 64, 138, 138]             128\n",
            "             ReLU-22         [-1, 64, 138, 138]               0\n",
            "           Conv2d-23        [-1, 256, 138, 138]          16,384\n",
            "      BatchNorm2d-24        [-1, 256, 138, 138]             512\n",
            "             ReLU-25        [-1, 256, 138, 138]               0\n",
            "       Bottleneck-26        [-1, 256, 138, 138]               0\n",
            "           Conv2d-27         [-1, 64, 138, 138]          16,384\n",
            "      BatchNorm2d-28         [-1, 64, 138, 138]             128\n",
            "             ReLU-29         [-1, 64, 138, 138]               0\n",
            "           Conv2d-30         [-1, 64, 138, 138]          36,864\n",
            "      BatchNorm2d-31         [-1, 64, 138, 138]             128\n",
            "             ReLU-32         [-1, 64, 138, 138]               0\n",
            "           Conv2d-33        [-1, 256, 138, 138]          16,384\n",
            "      BatchNorm2d-34        [-1, 256, 138, 138]             512\n",
            "             ReLU-35        [-1, 256, 138, 138]               0\n",
            "       Bottleneck-36        [-1, 256, 138, 138]               0\n",
            "           Conv2d-37        [-1, 128, 138, 138]          32,768\n",
            "      BatchNorm2d-38        [-1, 128, 138, 138]             256\n",
            "             ReLU-39        [-1, 128, 138, 138]               0\n",
            "           Conv2d-40          [-1, 128, 69, 69]         147,456\n",
            "      BatchNorm2d-41          [-1, 128, 69, 69]             256\n",
            "             ReLU-42          [-1, 128, 69, 69]               0\n",
            "           Conv2d-43          [-1, 512, 69, 69]          65,536\n",
            "      BatchNorm2d-44          [-1, 512, 69, 69]           1,024\n",
            "           Conv2d-45          [-1, 512, 69, 69]         131,072\n",
            "      BatchNorm2d-46          [-1, 512, 69, 69]           1,024\n",
            "             ReLU-47          [-1, 512, 69, 69]               0\n",
            "       Bottleneck-48          [-1, 512, 69, 69]               0\n",
            "           Conv2d-49          [-1, 128, 69, 69]          65,536\n",
            "      BatchNorm2d-50          [-1, 128, 69, 69]             256\n",
            "             ReLU-51          [-1, 128, 69, 69]               0\n",
            "           Conv2d-52          [-1, 128, 69, 69]         147,456\n",
            "      BatchNorm2d-53          [-1, 128, 69, 69]             256\n",
            "             ReLU-54          [-1, 128, 69, 69]               0\n",
            "           Conv2d-55          [-1, 512, 69, 69]          65,536\n",
            "      BatchNorm2d-56          [-1, 512, 69, 69]           1,024\n",
            "             ReLU-57          [-1, 512, 69, 69]               0\n",
            "       Bottleneck-58          [-1, 512, 69, 69]               0\n",
            "           Conv2d-59          [-1, 128, 69, 69]          65,536\n",
            "      BatchNorm2d-60          [-1, 128, 69, 69]             256\n",
            "             ReLU-61          [-1, 128, 69, 69]               0\n",
            "           Conv2d-62          [-1, 128, 69, 69]         147,456\n",
            "      BatchNorm2d-63          [-1, 128, 69, 69]             256\n",
            "             ReLU-64          [-1, 128, 69, 69]               0\n",
            "           Conv2d-65          [-1, 512, 69, 69]          65,536\n",
            "      BatchNorm2d-66          [-1, 512, 69, 69]           1,024\n",
            "             ReLU-67          [-1, 512, 69, 69]               0\n",
            "       Bottleneck-68          [-1, 512, 69, 69]               0\n",
            "           Conv2d-69          [-1, 128, 69, 69]          65,536\n",
            "      BatchNorm2d-70          [-1, 128, 69, 69]             256\n",
            "             ReLU-71          [-1, 128, 69, 69]               0\n",
            "           Conv2d-72          [-1, 128, 69, 69]         147,456\n",
            "      BatchNorm2d-73          [-1, 128, 69, 69]             256\n",
            "             ReLU-74          [-1, 128, 69, 69]               0\n",
            "           Conv2d-75          [-1, 512, 69, 69]          65,536\n",
            "      BatchNorm2d-76          [-1, 512, 69, 69]           1,024\n",
            "             ReLU-77          [-1, 512, 69, 69]               0\n",
            "       Bottleneck-78          [-1, 512, 69, 69]               0\n",
            "           Conv2d-79          [-1, 256, 69, 69]         131,072\n",
            "      BatchNorm2d-80          [-1, 256, 69, 69]             512\n",
            "             ReLU-81          [-1, 256, 69, 69]               0\n",
            "           Conv2d-82          [-1, 256, 35, 35]         589,824\n",
            "      BatchNorm2d-83          [-1, 256, 35, 35]             512\n",
            "             ReLU-84          [-1, 256, 35, 35]               0\n",
            "           Conv2d-85         [-1, 1024, 35, 35]         262,144\n",
            "      BatchNorm2d-86         [-1, 1024, 35, 35]           2,048\n",
            "           Conv2d-87         [-1, 1024, 35, 35]         524,288\n",
            "      BatchNorm2d-88         [-1, 1024, 35, 35]           2,048\n",
            "             ReLU-89         [-1, 1024, 35, 35]               0\n",
            "       Bottleneck-90         [-1, 1024, 35, 35]               0\n",
            "           Conv2d-91          [-1, 256, 35, 35]         262,144\n",
            "      BatchNorm2d-92          [-1, 256, 35, 35]             512\n",
            "             ReLU-93          [-1, 256, 35, 35]               0\n",
            "           Conv2d-94          [-1, 256, 35, 35]         589,824\n",
            "      BatchNorm2d-95          [-1, 256, 35, 35]             512\n",
            "             ReLU-96          [-1, 256, 35, 35]               0\n",
            "           Conv2d-97         [-1, 1024, 35, 35]         262,144\n",
            "      BatchNorm2d-98         [-1, 1024, 35, 35]           2,048\n",
            "             ReLU-99         [-1, 1024, 35, 35]               0\n",
            "      Bottleneck-100         [-1, 1024, 35, 35]               0\n",
            "          Conv2d-101          [-1, 256, 35, 35]         262,144\n",
            "     BatchNorm2d-102          [-1, 256, 35, 35]             512\n",
            "            ReLU-103          [-1, 256, 35, 35]               0\n",
            "          Conv2d-104          [-1, 256, 35, 35]         589,824\n",
            "     BatchNorm2d-105          [-1, 256, 35, 35]             512\n",
            "            ReLU-106          [-1, 256, 35, 35]               0\n",
            "          Conv2d-107         [-1, 1024, 35, 35]         262,144\n",
            "     BatchNorm2d-108         [-1, 1024, 35, 35]           2,048\n",
            "            ReLU-109         [-1, 1024, 35, 35]               0\n",
            "      Bottleneck-110         [-1, 1024, 35, 35]               0\n",
            "          Conv2d-111          [-1, 256, 35, 35]         262,144\n",
            "     BatchNorm2d-112          [-1, 256, 35, 35]             512\n",
            "            ReLU-113          [-1, 256, 35, 35]               0\n",
            "          Conv2d-114          [-1, 256, 35, 35]         589,824\n",
            "     BatchNorm2d-115          [-1, 256, 35, 35]             512\n",
            "            ReLU-116          [-1, 256, 35, 35]               0\n",
            "          Conv2d-117         [-1, 1024, 35, 35]         262,144\n",
            "     BatchNorm2d-118         [-1, 1024, 35, 35]           2,048\n",
            "            ReLU-119         [-1, 1024, 35, 35]               0\n",
            "      Bottleneck-120         [-1, 1024, 35, 35]               0\n",
            "          Conv2d-121          [-1, 256, 35, 35]         262,144\n",
            "     BatchNorm2d-122          [-1, 256, 35, 35]             512\n",
            "            ReLU-123          [-1, 256, 35, 35]               0\n",
            "          Conv2d-124          [-1, 256, 35, 35]         589,824\n",
            "     BatchNorm2d-125          [-1, 256, 35, 35]             512\n",
            "            ReLU-126          [-1, 256, 35, 35]               0\n",
            "          Conv2d-127         [-1, 1024, 35, 35]         262,144\n",
            "     BatchNorm2d-128         [-1, 1024, 35, 35]           2,048\n",
            "            ReLU-129         [-1, 1024, 35, 35]               0\n",
            "      Bottleneck-130         [-1, 1024, 35, 35]               0\n",
            "          Conv2d-131          [-1, 256, 35, 35]         262,144\n",
            "     BatchNorm2d-132          [-1, 256, 35, 35]             512\n",
            "            ReLU-133          [-1, 256, 35, 35]               0\n",
            "          Conv2d-134          [-1, 256, 35, 35]         589,824\n",
            "     BatchNorm2d-135          [-1, 256, 35, 35]             512\n",
            "            ReLU-136          [-1, 256, 35, 35]               0\n",
            "          Conv2d-137         [-1, 1024, 35, 35]         262,144\n",
            "     BatchNorm2d-138         [-1, 1024, 35, 35]           2,048\n",
            "            ReLU-139         [-1, 1024, 35, 35]               0\n",
            "      Bottleneck-140         [-1, 1024, 35, 35]               0\n",
            "          Conv2d-141          [-1, 256, 35, 35]         262,144\n",
            "     BatchNorm2d-142          [-1, 256, 35, 35]             512\n",
            "            ReLU-143          [-1, 256, 35, 35]               0\n",
            "          Conv2d-144          [-1, 256, 35, 35]         589,824\n",
            "     BatchNorm2d-145          [-1, 256, 35, 35]             512\n",
            "            ReLU-146          [-1, 256, 35, 35]               0\n",
            "          Conv2d-147         [-1, 1024, 35, 35]         262,144\n",
            "     BatchNorm2d-148         [-1, 1024, 35, 35]           2,048\n",
            "            ReLU-149         [-1, 1024, 35, 35]               0\n",
            "      Bottleneck-150         [-1, 1024, 35, 35]               0\n",
            "          Conv2d-151          [-1, 256, 35, 35]         262,144\n",
            "     BatchNorm2d-152          [-1, 256, 35, 35]             512\n",
            "            ReLU-153          [-1, 256, 35, 35]               0\n",
            "          Conv2d-154          [-1, 256, 35, 35]         589,824\n",
            "     BatchNorm2d-155          [-1, 256, 35, 35]             512\n",
            "            ReLU-156          [-1, 256, 35, 35]               0\n",
            "          Conv2d-157         [-1, 1024, 35, 35]         262,144\n",
            "     BatchNorm2d-158         [-1, 1024, 35, 35]           2,048\n",
            "            ReLU-159         [-1, 1024, 35, 35]               0\n",
            "      Bottleneck-160         [-1, 1024, 35, 35]               0\n",
            "          Conv2d-161          [-1, 256, 35, 35]         262,144\n",
            "     BatchNorm2d-162          [-1, 256, 35, 35]             512\n",
            "            ReLU-163          [-1, 256, 35, 35]               0\n",
            "          Conv2d-164          [-1, 256, 35, 35]         589,824\n",
            "     BatchNorm2d-165          [-1, 256, 35, 35]             512\n",
            "            ReLU-166          [-1, 256, 35, 35]               0\n",
            "          Conv2d-167         [-1, 1024, 35, 35]         262,144\n",
            "     BatchNorm2d-168         [-1, 1024, 35, 35]           2,048\n",
            "            ReLU-169         [-1, 1024, 35, 35]               0\n",
            "      Bottleneck-170         [-1, 1024, 35, 35]               0\n",
            "          Conv2d-171          [-1, 256, 35, 35]         262,144\n",
            "     BatchNorm2d-172          [-1, 256, 35, 35]             512\n",
            "            ReLU-173          [-1, 256, 35, 35]               0\n",
            "          Conv2d-174          [-1, 256, 35, 35]         589,824\n",
            "     BatchNorm2d-175          [-1, 256, 35, 35]             512\n",
            "            ReLU-176          [-1, 256, 35, 35]               0\n",
            "          Conv2d-177         [-1, 1024, 35, 35]         262,144\n",
            "     BatchNorm2d-178         [-1, 1024, 35, 35]           2,048\n",
            "            ReLU-179         [-1, 1024, 35, 35]               0\n",
            "      Bottleneck-180         [-1, 1024, 35, 35]               0\n",
            "          Conv2d-181          [-1, 256, 35, 35]         262,144\n",
            "     BatchNorm2d-182          [-1, 256, 35, 35]             512\n",
            "            ReLU-183          [-1, 256, 35, 35]               0\n",
            "          Conv2d-184          [-1, 256, 35, 35]         589,824\n",
            "     BatchNorm2d-185          [-1, 256, 35, 35]             512\n",
            "            ReLU-186          [-1, 256, 35, 35]               0\n",
            "          Conv2d-187         [-1, 1024, 35, 35]         262,144\n",
            "     BatchNorm2d-188         [-1, 1024, 35, 35]           2,048\n",
            "            ReLU-189         [-1, 1024, 35, 35]               0\n",
            "      Bottleneck-190         [-1, 1024, 35, 35]               0\n",
            "          Conv2d-191          [-1, 256, 35, 35]         262,144\n",
            "     BatchNorm2d-192          [-1, 256, 35, 35]             512\n",
            "            ReLU-193          [-1, 256, 35, 35]               0\n",
            "          Conv2d-194          [-1, 256, 35, 35]         589,824\n",
            "     BatchNorm2d-195          [-1, 256, 35, 35]             512\n",
            "            ReLU-196          [-1, 256, 35, 35]               0\n",
            "          Conv2d-197         [-1, 1024, 35, 35]         262,144\n",
            "     BatchNorm2d-198         [-1, 1024, 35, 35]           2,048\n",
            "            ReLU-199         [-1, 1024, 35, 35]               0\n",
            "      Bottleneck-200         [-1, 1024, 35, 35]               0\n",
            "          Conv2d-201          [-1, 256, 35, 35]         262,144\n",
            "     BatchNorm2d-202          [-1, 256, 35, 35]             512\n",
            "            ReLU-203          [-1, 256, 35, 35]               0\n",
            "          Conv2d-204          [-1, 256, 35, 35]         589,824\n",
            "     BatchNorm2d-205          [-1, 256, 35, 35]             512\n",
            "            ReLU-206          [-1, 256, 35, 35]               0\n",
            "          Conv2d-207         [-1, 1024, 35, 35]         262,144\n",
            "     BatchNorm2d-208         [-1, 1024, 35, 35]           2,048\n",
            "            ReLU-209         [-1, 1024, 35, 35]               0\n",
            "      Bottleneck-210         [-1, 1024, 35, 35]               0\n",
            "          Conv2d-211          [-1, 256, 35, 35]         262,144\n",
            "     BatchNorm2d-212          [-1, 256, 35, 35]             512\n",
            "            ReLU-213          [-1, 256, 35, 35]               0\n",
            "          Conv2d-214          [-1, 256, 35, 35]         589,824\n",
            "     BatchNorm2d-215          [-1, 256, 35, 35]             512\n",
            "            ReLU-216          [-1, 256, 35, 35]               0\n",
            "          Conv2d-217         [-1, 1024, 35, 35]         262,144\n",
            "     BatchNorm2d-218         [-1, 1024, 35, 35]           2,048\n",
            "            ReLU-219         [-1, 1024, 35, 35]               0\n",
            "      Bottleneck-220         [-1, 1024, 35, 35]               0\n",
            "          Conv2d-221          [-1, 256, 35, 35]         262,144\n",
            "     BatchNorm2d-222          [-1, 256, 35, 35]             512\n",
            "            ReLU-223          [-1, 256, 35, 35]               0\n",
            "          Conv2d-224          [-1, 256, 35, 35]         589,824\n",
            "     BatchNorm2d-225          [-1, 256, 35, 35]             512\n",
            "            ReLU-226          [-1, 256, 35, 35]               0\n",
            "          Conv2d-227         [-1, 1024, 35, 35]         262,144\n",
            "     BatchNorm2d-228         [-1, 1024, 35, 35]           2,048\n",
            "            ReLU-229         [-1, 1024, 35, 35]               0\n",
            "      Bottleneck-230         [-1, 1024, 35, 35]               0\n",
            "          Conv2d-231          [-1, 256, 35, 35]         262,144\n",
            "     BatchNorm2d-232          [-1, 256, 35, 35]             512\n",
            "            ReLU-233          [-1, 256, 35, 35]               0\n",
            "          Conv2d-234          [-1, 256, 35, 35]         589,824\n",
            "     BatchNorm2d-235          [-1, 256, 35, 35]             512\n",
            "            ReLU-236          [-1, 256, 35, 35]               0\n",
            "          Conv2d-237         [-1, 1024, 35, 35]         262,144\n",
            "     BatchNorm2d-238         [-1, 1024, 35, 35]           2,048\n",
            "            ReLU-239         [-1, 1024, 35, 35]               0\n",
            "      Bottleneck-240         [-1, 1024, 35, 35]               0\n",
            "          Conv2d-241          [-1, 256, 35, 35]         262,144\n",
            "     BatchNorm2d-242          [-1, 256, 35, 35]             512\n",
            "            ReLU-243          [-1, 256, 35, 35]               0\n",
            "          Conv2d-244          [-1, 256, 35, 35]         589,824\n",
            "     BatchNorm2d-245          [-1, 256, 35, 35]             512\n",
            "            ReLU-246          [-1, 256, 35, 35]               0\n",
            "          Conv2d-247         [-1, 1024, 35, 35]         262,144\n",
            "     BatchNorm2d-248         [-1, 1024, 35, 35]           2,048\n",
            "            ReLU-249         [-1, 1024, 35, 35]               0\n",
            "      Bottleneck-250         [-1, 1024, 35, 35]               0\n",
            "          Conv2d-251          [-1, 256, 35, 35]         262,144\n",
            "     BatchNorm2d-252          [-1, 256, 35, 35]             512\n",
            "            ReLU-253          [-1, 256, 35, 35]               0\n",
            "          Conv2d-254          [-1, 256, 35, 35]         589,824\n",
            "     BatchNorm2d-255          [-1, 256, 35, 35]             512\n",
            "            ReLU-256          [-1, 256, 35, 35]               0\n",
            "          Conv2d-257         [-1, 1024, 35, 35]         262,144\n",
            "     BatchNorm2d-258         [-1, 1024, 35, 35]           2,048\n",
            "            ReLU-259         [-1, 1024, 35, 35]               0\n",
            "      Bottleneck-260         [-1, 1024, 35, 35]               0\n",
            "          Conv2d-261          [-1, 256, 35, 35]         262,144\n",
            "     BatchNorm2d-262          [-1, 256, 35, 35]             512\n",
            "            ReLU-263          [-1, 256, 35, 35]               0\n",
            "          Conv2d-264          [-1, 256, 35, 35]         589,824\n",
            "     BatchNorm2d-265          [-1, 256, 35, 35]             512\n",
            "            ReLU-266          [-1, 256, 35, 35]               0\n",
            "          Conv2d-267         [-1, 1024, 35, 35]         262,144\n",
            "     BatchNorm2d-268         [-1, 1024, 35, 35]           2,048\n",
            "            ReLU-269         [-1, 1024, 35, 35]               0\n",
            "      Bottleneck-270         [-1, 1024, 35, 35]               0\n",
            "          Conv2d-271          [-1, 256, 35, 35]         262,144\n",
            "     BatchNorm2d-272          [-1, 256, 35, 35]             512\n",
            "            ReLU-273          [-1, 256, 35, 35]               0\n",
            "          Conv2d-274          [-1, 256, 35, 35]         589,824\n",
            "     BatchNorm2d-275          [-1, 256, 35, 35]             512\n",
            "            ReLU-276          [-1, 256, 35, 35]               0\n",
            "          Conv2d-277         [-1, 1024, 35, 35]         262,144\n",
            "     BatchNorm2d-278         [-1, 1024, 35, 35]           2,048\n",
            "            ReLU-279         [-1, 1024, 35, 35]               0\n",
            "      Bottleneck-280         [-1, 1024, 35, 35]               0\n",
            "          Conv2d-281          [-1, 256, 35, 35]         262,144\n",
            "     BatchNorm2d-282          [-1, 256, 35, 35]             512\n",
            "            ReLU-283          [-1, 256, 35, 35]               0\n",
            "          Conv2d-284          [-1, 256, 35, 35]         589,824\n",
            "     BatchNorm2d-285          [-1, 256, 35, 35]             512\n",
            "            ReLU-286          [-1, 256, 35, 35]               0\n",
            "          Conv2d-287         [-1, 1024, 35, 35]         262,144\n",
            "     BatchNorm2d-288         [-1, 1024, 35, 35]           2,048\n",
            "            ReLU-289         [-1, 1024, 35, 35]               0\n",
            "      Bottleneck-290         [-1, 1024, 35, 35]               0\n",
            "          Conv2d-291          [-1, 256, 35, 35]         262,144\n",
            "     BatchNorm2d-292          [-1, 256, 35, 35]             512\n",
            "            ReLU-293          [-1, 256, 35, 35]               0\n",
            "          Conv2d-294          [-1, 256, 35, 35]         589,824\n",
            "     BatchNorm2d-295          [-1, 256, 35, 35]             512\n",
            "            ReLU-296          [-1, 256, 35, 35]               0\n",
            "          Conv2d-297         [-1, 1024, 35, 35]         262,144\n",
            "     BatchNorm2d-298         [-1, 1024, 35, 35]           2,048\n",
            "            ReLU-299         [-1, 1024, 35, 35]               0\n",
            "      Bottleneck-300         [-1, 1024, 35, 35]               0\n",
            "          Conv2d-301          [-1, 256, 35, 35]         262,144\n",
            "     BatchNorm2d-302          [-1, 256, 35, 35]             512\n",
            "            ReLU-303          [-1, 256, 35, 35]               0\n",
            "          Conv2d-304          [-1, 256, 35, 35]         589,824\n",
            "     BatchNorm2d-305          [-1, 256, 35, 35]             512\n",
            "            ReLU-306          [-1, 256, 35, 35]               0\n",
            "          Conv2d-307         [-1, 1024, 35, 35]         262,144\n",
            "     BatchNorm2d-308         [-1, 1024, 35, 35]           2,048\n",
            "            ReLU-309         [-1, 1024, 35, 35]               0\n",
            "      Bottleneck-310         [-1, 1024, 35, 35]               0\n",
            "          Conv2d-311          [-1, 512, 35, 35]         524,288\n",
            "     BatchNorm2d-312          [-1, 512, 35, 35]           1,024\n",
            "            ReLU-313          [-1, 512, 35, 35]               0\n",
            "          Conv2d-314          [-1, 512, 18, 18]       2,359,296\n",
            "     BatchNorm2d-315          [-1, 512, 18, 18]           1,024\n",
            "            ReLU-316          [-1, 512, 18, 18]               0\n",
            "          Conv2d-317         [-1, 2048, 18, 18]       1,048,576\n",
            "     BatchNorm2d-318         [-1, 2048, 18, 18]           4,096\n",
            "          Conv2d-319         [-1, 2048, 18, 18]       2,097,152\n",
            "     BatchNorm2d-320         [-1, 2048, 18, 18]           4,096\n",
            "            ReLU-321         [-1, 2048, 18, 18]               0\n",
            "      Bottleneck-322         [-1, 2048, 18, 18]               0\n",
            "          Conv2d-323          [-1, 512, 18, 18]       1,048,576\n",
            "     BatchNorm2d-324          [-1, 512, 18, 18]           1,024\n",
            "            ReLU-325          [-1, 512, 18, 18]               0\n",
            "          Conv2d-326          [-1, 512, 18, 18]       2,359,296\n",
            "     BatchNorm2d-327          [-1, 512, 18, 18]           1,024\n",
            "            ReLU-328          [-1, 512, 18, 18]               0\n",
            "          Conv2d-329         [-1, 2048, 18, 18]       1,048,576\n",
            "     BatchNorm2d-330         [-1, 2048, 18, 18]           4,096\n",
            "            ReLU-331         [-1, 2048, 18, 18]               0\n",
            "      Bottleneck-332         [-1, 2048, 18, 18]               0\n",
            "          Conv2d-333          [-1, 512, 18, 18]       1,048,576\n",
            "     BatchNorm2d-334          [-1, 512, 18, 18]           1,024\n",
            "            ReLU-335          [-1, 512, 18, 18]               0\n",
            "          Conv2d-336          [-1, 512, 18, 18]       2,359,296\n",
            "     BatchNorm2d-337          [-1, 512, 18, 18]           1,024\n",
            "            ReLU-338          [-1, 512, 18, 18]               0\n",
            "          Conv2d-339         [-1, 2048, 18, 18]       1,048,576\n",
            "     BatchNorm2d-340         [-1, 2048, 18, 18]           4,096\n",
            "            ReLU-341         [-1, 2048, 18, 18]               0\n",
            "      Bottleneck-342         [-1, 2048, 18, 18]               0\n",
            "================================================================\n",
            "Total params: 42,500,160\n",
            "Trainable params: 42,500,160\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 3.46\n",
            "Forward/backward pass size (MB): 2651.10\n",
            "Params size (MB): 162.13\n",
            "Estimated Total Size (MB): 2816.69\n",
            "----------------------------------------------------------------\n",
            "backbone output feature 개수 : 4\n",
            "C2 output shape :  torch.Size([1, 256, 138, 138])\n",
            "C3 output shape :  torch.Size([1, 512, 69, 69])\n",
            "C4 output shape :  torch.Size([1, 1024, 35, 35])\n",
            "C5 output shape :  torch.Size([1, 2048, 18, 18])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aXglib7koGS",
        "colab_type": "code",
        "outputId": "38c1ebff-6725-490f-82e3-4a3384cd74bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "############### Feature Pyramid Network ###############\n",
        "class FPN(nn.Module):\n",
        "    \"\"\"\n",
        "    Implemented FPN here is different from the FPN introduced in https://arxiv.org/pdf/1612.03144.pdf.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels):\n",
        "        '''\n",
        "        in_channels=[512, 1024, 2048]\n",
        "        '''\n",
        "        super().__init__()\n",
        "        self.num_downsample = 2\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        self.last_layers = nn.ModuleList([nn.Conv2d(x, 256, kernel_size=1) for x in reversed(self.in_channels)])\n",
        "        # 1 x 1 conv to backbone feature map\n",
        "        # ModuleList((0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
        "        #            (1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
        "        #            (2): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1)))\n",
        "\n",
        "        self.final_layers = nn.ModuleList([nn.Conv2d(256, 256, kernel_size=3, padding=1) for _ in self.in_channels])\n",
        "        # 3 x 3 conv to FPN feature map in order to recover error that might be occur during upsampling \n",
        "        # and add two different feature map\n",
        "        # ModuleList((0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        #            (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        #            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)))\n",
        "\n",
        "        self.downsample_layers = nn.ModuleList([nn.Conv2d(256, 256, kernel_size=3, padding=1, stride=2)\n",
        "                                                for _ in range(self.num_downsample)])\n",
        "        # 3 x 3 conv to P5 in order to make P6, P7 final feature map\n",
        "        # ModuleList((0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
        "        #            (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)))\n",
        "\n",
        "    def forward(self, backbone_outs):\n",
        "        '''\n",
        "        #backbone_outs = [[n, 512, 69, 69], [n, 1024, 35, 35], [n, 2048, 18, 18]]\n",
        "        In class Yolact's train(), remove C2 from bakebone_outs. So FPN gets three feature outs.\n",
        "        '''\n",
        "        out = []\n",
        "        x = torch.zeros(1, device=backbone_outs[0].device)\n",
        "        for i in range(len(backbone_outs)):\n",
        "            out.append(x)\n",
        "\n",
        "        # For backward compatability, the conv layers are stored in reverse but the input and output is\n",
        "        # given in the correct order. Thus, use j=-i-1 for the input and output and i for the conv layers.\n",
        "        j = len(backbone_outs)  # convouts: C3, C4, C5\n",
        "\n",
        "        for last_layer in self.last_layers:\n",
        "            j -= 1\n",
        "            if j < len(backbone_outs) - 1:\n",
        "                #backbone_outs = [[n, 512, 69, 69], [n, 1024, 35, 35], [n, 2048, 18, 18]]\n",
        "                _, _, h, w = backbone_outs[j].size()\n",
        "                x = F.interpolate(x, size=(h, w), mode='bilinear', align_corners=False)\n",
        "            x = x + last_layer(backbone_outs[j])\n",
        "            out[j] = x\n",
        "        j = len(backbone_outs)\n",
        "        for final_layer in self.final_layers:\n",
        "            j -= 1\n",
        "            out[j] = F.relu(final_layer(out[j]))\n",
        "\n",
        "        for layer in self.downsample_layers:\n",
        "            out.append(layer(out[-1]))\n",
        "\n",
        "        return out\n",
        "backbone_outs=[]\n",
        "for i in backbone_selected_layers :\n",
        "    backbone_outs.append(backbone[i])\n",
        "\n",
        "fpn = FPN([512, 1024, 2048])\n",
        "fpn_outs=fpn(backbone_outs)\n",
        "print('FPN output feature 개수 :', len(fpn_outs))\n",
        "print('P3 output shape : ', fpn_outs[0].shape)\n",
        "print('P4 output shape : ', fpn_outs[1].shape)\n",
        "print('P5 output shape : ', fpn_outs[2].shape)\n",
        "print('P6 output shape : ', fpn_outs[3].shape)\n",
        "print('P7 output shape : ', fpn_outs[4].shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FPN output feature 개수 : 5\n",
            "P3 output shape :  torch.Size([1, 256, 69, 69])\n",
            "P4 output shape :  torch.Size([1, 256, 35, 35])\n",
            "P5 output shape :  torch.Size([1, 256, 18, 18])\n",
            "P6 output shape :  torch.Size([1, 256, 9, 9])\n",
            "P7 output shape :  torch.Size([1, 256, 5, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5L5HZ9JIZsk",
        "colab_type": "code",
        "outputId": "53215649-4ea5-44bf-9b85-588cb7524727",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 953
        }
      },
      "source": [
        "############### Proto Network ###############\n",
        "'''\n",
        "Use P3 which is deepest feature map and has highest resolution\n",
        "'''\n",
        "mask_proto_net = [(256, 3, {'padding': 1}), (256, 3, {'padding': 1}), (256, 3, {'padding': 1}),\n",
        "                  (None, -2, {}), (256, 3, {'padding': 1}), (32, 1, {})]\n",
        "\n",
        "class Protonet(nn.Module) :\n",
        "    def __init__(self, mask_proto_net) :\n",
        "        super().__init__()\n",
        "\n",
        "        self.inplanes=256\n",
        "        self.mask_proto_net = mask_proto_net\n",
        "        self.conv1 = nn.Conv2d(self.inplanes, mask_proto_net[0][0], kernel_size=mask_proto_net[0][1], **mask_proto_net[0][2])\n",
        "        self.conv2 = nn.Conv2d(self.inplanes, mask_proto_net[1][0], kernel_size=mask_proto_net[1][1], **mask_proto_net[1][2])\n",
        "        self.conv3 = nn.Conv2d(self.inplanes, mask_proto_net[2][0], kernel_size=mask_proto_net[2][1], **mask_proto_net[2][2])\n",
        "        self.conv4 = nn.Conv2d(self.inplanes, mask_proto_net[4][0], kernel_size=mask_proto_net[4][1], **mask_proto_net[4][2])\n",
        "        self.conv5 = nn.Conv2d(self.inplanes, mask_proto_net[5][0], kernel_size=mask_proto_net[5][1], **mask_proto_net[5][2])\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv3(out)\n",
        "        out = self.relu(out)\n",
        "        out = F.interpolate(out, scale_factor = -self.mask_proto_net[3][1], mode='bilinear', align_corners=False, **self.mask_proto_net[3][2])\n",
        "        out = self.relu(out)\n",
        "        out = self.conv4(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv5(out)\n",
        "        \n",
        "        \n",
        "        return out\n",
        "\n",
        "proto_out=Protonet(mask_proto_net)(fpn_outs[0])\n",
        "print(proto_out)\n",
        "print('-'*50)\n",
        "print('Proto net output shape : ', proto_out.shape)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[[ 0.0395,  0.0383,  0.0426,  ...,  0.0419,  0.0401,  0.0425],\n",
            "          [ 0.0403,  0.0362,  0.0417,  ...,  0.0394,  0.0360,  0.0411],\n",
            "          [ 0.0415,  0.0369,  0.0400,  ...,  0.0424,  0.0399,  0.0429],\n",
            "          ...,\n",
            "          [ 0.0405,  0.0404,  0.0444,  ...,  0.0406,  0.0408,  0.0414],\n",
            "          [ 0.0405,  0.0413,  0.0433,  ...,  0.0394,  0.0400,  0.0381],\n",
            "          [ 0.0388,  0.0386,  0.0365,  ...,  0.0365,  0.0365,  0.0363]],\n",
            "\n",
            "         [[ 0.0315,  0.0320,  0.0326,  ...,  0.0331,  0.0317,  0.0288],\n",
            "          [ 0.0345,  0.0293,  0.0281,  ...,  0.0315,  0.0292,  0.0286],\n",
            "          [ 0.0368,  0.0326,  0.0332,  ...,  0.0340,  0.0318,  0.0309],\n",
            "          ...,\n",
            "          [ 0.0449,  0.0434,  0.0444,  ...,  0.0381,  0.0375,  0.0356],\n",
            "          [ 0.0440,  0.0452,  0.0460,  ...,  0.0369,  0.0365,  0.0356],\n",
            "          [ 0.0391,  0.0424,  0.0442,  ...,  0.0366,  0.0374,  0.0340]],\n",
            "\n",
            "         [[-0.0486, -0.0499, -0.0515,  ..., -0.0495, -0.0501, -0.0513],\n",
            "          [-0.0465, -0.0454, -0.0472,  ..., -0.0492, -0.0506, -0.0526],\n",
            "          [-0.0471, -0.0476, -0.0495,  ..., -0.0506, -0.0514, -0.0545],\n",
            "          ...,\n",
            "          [-0.0436, -0.0492, -0.0490,  ..., -0.0456, -0.0483, -0.0528],\n",
            "          [-0.0448, -0.0487, -0.0487,  ..., -0.0462, -0.0489, -0.0532],\n",
            "          [-0.0458, -0.0446, -0.0448,  ..., -0.0481, -0.0495, -0.0503]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0433,  0.0443,  0.0466,  ...,  0.0427,  0.0424,  0.0409],\n",
            "          [ 0.0479,  0.0518,  0.0539,  ...,  0.0448,  0.0452,  0.0390],\n",
            "          [ 0.0500,  0.0551,  0.0569,  ...,  0.0449,  0.0459,  0.0380],\n",
            "          ...,\n",
            "          [ 0.0491,  0.0502,  0.0498,  ...,  0.0379,  0.0388,  0.0356],\n",
            "          [ 0.0466,  0.0478,  0.0475,  ...,  0.0401,  0.0414,  0.0366],\n",
            "          [ 0.0393,  0.0365,  0.0347,  ...,  0.0349,  0.0364,  0.0341]],\n",
            "\n",
            "         [[ 0.0417,  0.0376,  0.0369,  ...,  0.0392,  0.0381,  0.0400],\n",
            "          [ 0.0416,  0.0365,  0.0376,  ...,  0.0406,  0.0387,  0.0392],\n",
            "          [ 0.0409,  0.0357,  0.0373,  ...,  0.0376,  0.0367,  0.0399],\n",
            "          ...,\n",
            "          [ 0.0446,  0.0473,  0.0467,  ...,  0.0431,  0.0434,  0.0393],\n",
            "          [ 0.0455,  0.0469,  0.0465,  ...,  0.0397,  0.0397,  0.0388],\n",
            "          [ 0.0419,  0.0406,  0.0407,  ...,  0.0358,  0.0333,  0.0345]],\n",
            "\n",
            "         [[ 0.0486,  0.0480,  0.0476,  ...,  0.0427,  0.0421,  0.0429],\n",
            "          [ 0.0497,  0.0475,  0.0453,  ...,  0.0405,  0.0384,  0.0403],\n",
            "          [ 0.0510,  0.0501,  0.0481,  ...,  0.0395,  0.0369,  0.0394],\n",
            "          ...,\n",
            "          [ 0.0465,  0.0465,  0.0503,  ...,  0.0426,  0.0423,  0.0402],\n",
            "          [ 0.0480,  0.0444,  0.0475,  ...,  0.0437,  0.0427,  0.0400],\n",
            "          [ 0.0452,  0.0422,  0.0437,  ...,  0.0412,  0.0407,  0.0421]]]],\n",
            "       grad_fn=<MkldnnConvolutionBackward>)\n",
            "--------------------------------------------------\n",
            "Proto net output shape :  torch.Size([1, 32, 138, 138])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4IaHJC_OQoJ",
        "colab_type": "code",
        "outputId": "c05f05e8-0f51-4dc6-89c7-4923a08c2676",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        }
      },
      "source": [
        "#proto_out : [n, 32, 138, 138]\n",
        "coef_dim=proto_out.shape[1]\n",
        "num_classes=81\n",
        "aspect_ratios: [1, 1 / 2, 2]\n",
        "class PredictionModule(nn.Module):\n",
        "    def __init__(self, in_channels, coef_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_classes = 81\n",
        "        self.coef_dim = coef_dim\n",
        "        self.num_priors = 3            # num of anchor box for each pixel of feature map\n",
        "\n",
        "        self.upfeature = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        \n",
        "        out_channels = 256\n",
        "        self.bbox_layer = nn.Conv2d(out_channels, self.num_priors * 4, kernel_size=3, padding=1)\n",
        "        self.conf_layer = nn.Conv2d(out_channels, self.num_priors * self.num_classes, kernel_size=3, padding=1)\n",
        "        self.mask_layer = nn.Conv2d(out_channels, self.num_priors * self.coef_dim, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.upfeature(x)\n",
        "        x = self.relu(x)\n",
        "        conf = self.conf_layer(x).permute(0, 2, 3, 1).contiguous().view(x.size(0), -1, self.num_classes)\n",
        "        bbox = self.bbox_layer(x).permute(0, 2, 3, 1).contiguous().view(x.size(0), -1, 4)\n",
        "        coef_test = self.mask_layer(x)\n",
        "        print('mask layer output shape : ', coef_test.shape)\n",
        "        coef = self.mask_layer(x).permute(0, 2, 3, 1).contiguous().view(x.size(0), -1, self.coef_dim)       \n",
        "        # mask_layer output shape : [n, 96, 69, 69] / In order to make it's shape [n, 69*69*3, 32], use permute and contiguous.\n",
        "        print('Changed shape : ', coef.shape)\n",
        "        coef = torch.tanh(coef)\n",
        "\n",
        "        return {'box': bbox, 'class': conf, 'coef': coef}\n",
        "prediction_layers = nn.ModuleList()\n",
        "prediction_layers.append(PredictionModule(in_channels=256, coef_dim=coef_dim))\n",
        "print(prediction_layers[0](fpn_outs[0]))\n",
        "\n",
        "predictions = {'box': [], 'class': [], 'coef': []}\n",
        "for i in range(len(fpn_outs)) :\n",
        "    p=prediction_layers[0](fpn_outs[i])\n",
        "    for key, value in p.items() :\n",
        "        predictions[key].append(value)\n",
        "print(predictions.keys())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mask layer output shape :  torch.Size([1, 96, 69, 69])\n",
            "Changed shape :  torch.Size([1, 14283, 32])\n",
            "{'box': tensor([[[-0.0679,  0.1455,  0.0270,  0.1836],\n",
            "         [-0.1596, -0.0462, -0.0232, -0.2538],\n",
            "         [-0.0641, -0.1218, -0.1471, -0.0621],\n",
            "         ...,\n",
            "         [ 0.0878,  0.2053,  0.0869,  0.1902],\n",
            "         [ 0.2527,  0.0430,  0.1544,  0.1631],\n",
            "         [ 0.0999,  0.0977,  0.1058,  0.1289]]], grad_fn=<ViewBackward>), 'class': tensor([[[-0.0593,  0.0791, -0.0490,  ...,  0.1516, -0.1004, -0.0774],\n",
            "         [-0.0414, -0.1331, -0.1768,  ...,  0.1356, -0.0410,  0.0604],\n",
            "         [ 0.0413,  0.1187,  0.0259,  ...,  0.0632,  0.0308, -0.1532],\n",
            "         ...,\n",
            "         [-0.0274, -0.0330, -0.1076,  ..., -0.1525, -0.0576, -0.0219],\n",
            "         [-0.0523, -0.0306,  0.1327,  ..., -0.0299, -0.0412,  0.0042],\n",
            "         [-0.1187,  0.1677,  0.0395,  ...,  0.1190, -0.0270, -0.0976]]],\n",
            "       grad_fn=<ViewBackward>), 'coef': tensor([[[-0.1319,  0.0452,  0.0812,  ...,  0.0077, -0.0303,  0.0068],\n",
            "         [ 0.0466, -0.0957,  0.0434,  ..., -0.1297,  0.0693,  0.0343],\n",
            "         [ 0.0909,  0.0334, -0.1158,  ..., -0.1050, -0.1844, -0.1340],\n",
            "         ...,\n",
            "         [ 0.0145, -0.0284, -0.0584,  ..., -0.2148,  0.0813,  0.1924],\n",
            "         [-0.0972, -0.1237,  0.0363,  ..., -0.0576, -0.0626, -0.0764],\n",
            "         [-0.1935, -0.0915,  0.1146,  ...,  0.0769, -0.0700,  0.0946]]],\n",
            "       grad_fn=<TanhBackward>)}\n",
            "mask layer output shape :  torch.Size([1, 96, 69, 69])\n",
            "Changed shape :  torch.Size([1, 14283, 32])\n",
            "mask layer output shape :  torch.Size([1, 96, 35, 35])\n",
            "Changed shape :  torch.Size([1, 3675, 32])\n",
            "mask layer output shape :  torch.Size([1, 96, 18, 18])\n",
            "Changed shape :  torch.Size([1, 972, 32])\n",
            "mask layer output shape :  torch.Size([1, 96, 9, 9])\n",
            "Changed shape :  torch.Size([1, 243, 32])\n",
            "mask layer output shape :  torch.Size([1, 96, 5, 5])\n",
            "Changed shape :  torch.Size([1, 75, 32])\n",
            "dict_keys(['box', 'class', 'coef'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MGWHOo-gi04",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def jaccard(box_a, box_b, iscrowd:bool=False):\n",
        "    \"\"\"\n",
        "    Return IoU between box_a and box_b.\n",
        "    \"\"\"\n",
        "    use_batch = True\n",
        "    if box_a.dim() == 2:\n",
        "        use_batch = False\n",
        "        box_a = box_a[None, ...]\n",
        "        box_b = box_b[None, ...]\n",
        "    \n",
        "    inter = intersect(box_a, box_b)\n",
        "    \n",
        "    area_a = ((box_a[:, :, 2]-box_a[:, :, 0]) *\n",
        "              (box_a[:, :, 3]-box_a[:, :, 1])).unsqueeze(2).expand_as(inter)  # [A,B]\n",
        "    area_b = ((box_b[:, :, 2]-box_b[:, :, 0]) *\n",
        "              (box_b[:, :, 3]-box_b[:, :, 1])).unsqueeze(1).expand_as(inter)  # [A,B]\n",
        "    union = area_a + area_b - inter\n",
        "    \n",
        "    out = inter / area_a if iscrowd else inter / union\n",
        "    \n",
        "    return out if use_batch else out.squeeze(0)\n",
        "def intersect(box_a, box_b):\n",
        "    \"\"\" \n",
        "    Return intersection area, Shape: [n,A,B].\n",
        "    In order to make size as [n, A, B], resize both tensors to [A,B,2]\n",
        "    [A,2] -> [A,1,2] -> [A,B,2]\n",
        "    [B,2] -> [1,B,2] -> [A,B,2]\n",
        "    Then compute the area of intersect between box_a and box_b.\n",
        "    \n",
        "    box_a: (tensor) bounding boxes, Shape: [n,A,4].\n",
        "    box_b: (tensor) bounding boxes, Shape: [n,B,4].\n",
        "    \n",
        "    \"\"\"\n",
        "    n = box_a.size(0)\n",
        "    A = box_a.size(1)\n",
        "    B = box_b.size(1)\n",
        "    max_xy = torch.min(box_a[:, :, 2:].unsqueeze(2).expand(n, A, B, 2),\n",
        "                       box_b[:, :, 2:].unsqueeze(1).expand(n, A, B, 2))\n",
        "    min_xy = torch.max(box_a[:, :, :2].unsqueeze(2).expand(n, A, B, 2),\n",
        "                       box_b[:, :, :2].unsqueeze(1).expand(n, A, B, 2))\n",
        "    inter = torch.clamp((max_xy - min_xy), min=0)\n",
        "    return inter[:, :, :, 0] * inter[:, :, :, 1]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FGOjnpezuZG",
        "colab_type": "code",
        "outputId": "0eb86a57-2c0e-4e22-a6a4-a2db8e7f7f44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        }
      },
      "source": [
        "conf_thresh=0.05\n",
        "max_num_detections=100\n",
        "def fast_nms(boxes, scores, iou_threshold:float=0.5, top_k:int=200, second_threshold:bool=False):\n",
        "        \n",
        "        scores, idx = scores.sort(1, descending=True)\n",
        "        \n",
        "        idx = idx[:, :top_k].contiguous()\n",
        "        scores = scores[:, :top_k]\n",
        "    \n",
        "        num_classes, num_dets = idx.size()\n",
        "        print('-'*80)\n",
        "\n",
        "        print('sorted index : ', idx)\n",
        "        boxes = boxes[idx.view(-1), :].view(num_classes, num_dets, 4)\n",
        "        #masks = masks[idx.view(-1), :].view(num_classes, num_dets, -1)\n",
        "        print('<sorted boxes>')\n",
        "        print(boxes)\n",
        "        iou = jaccard(boxes, boxes)\n",
        "        print('-'*80)\n",
        "        \n",
        "        iou.triu_(diagonal=1)\n",
        "        print(iou)\n",
        "        iou_max, _ = iou.max(dim=1)\n",
        "\n",
        "        # Now just filter out the ones higher than the threshold\n",
        "        keep = (iou_max <= iou_threshold)\n",
        "        print('keep: ', keep)\n",
        "        if second_threshold:\n",
        "            keep *= (scores > conf_thresh)\n",
        "\n",
        "        # Assign each kept detection to its corresponding class\n",
        "        classes = torch.arange(num_classes, device=boxes.device)[:, None].expand_as(keep)\n",
        "        classes = classes[keep]\n",
        "\n",
        "        boxes = boxes[keep]\n",
        "        #masks = masks[keep]\n",
        "        scores = scores[keep]\n",
        "        \n",
        "        # Only keep the top cfg.max_num_detections highest scores across all classes\n",
        "        scores, idx = scores.sort(0, descending=True)\n",
        "        idx = idx[:max_num_detections]\n",
        "        scores = scores[:max_num_detections]\n",
        "\n",
        "        classes = classes[idx]\n",
        "        boxes = boxes[idx]\n",
        "        #masks = masks[idx]\n",
        "\n",
        "        return boxes, classes, scores\n",
        "score=torch.FloatTensor([[3.12, 1.213, 4., 5.23]])\n",
        "print('<score>')\n",
        "print(score)\n",
        "temp=[[0., 0., 5., 5.], [1., 1., 5., 5.], [1., 1., 2., 2.], [4., 4., 7., 7.]]\n",
        "#temp=list(np.arange(i, i+4,) for i in range(4))\n",
        "temp=torch.FloatTensor(temp)\n",
        "print('')\n",
        "print('<box coordinate>')\n",
        "print(temp)\n",
        "b, c, s=fast_nms(temp, score)\n",
        "print('*'*80)\n",
        "print(b, c, s)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<score>\n",
            "tensor([[3.1200, 1.2130, 4.0000, 5.2300]])\n",
            "\n",
            "<box coordinate>\n",
            "tensor([[0., 0., 5., 5.],\n",
            "        [1., 1., 5., 5.],\n",
            "        [1., 1., 2., 2.],\n",
            "        [4., 4., 7., 7.]])\n",
            "--------------------------------------------------------------------------------\n",
            "sorted index :  tensor([[3, 2, 0, 1]])\n",
            "<sorted boxes>\n",
            "tensor([[[4., 4., 7., 7.],\n",
            "         [1., 1., 2., 2.],\n",
            "         [0., 0., 5., 5.],\n",
            "         [1., 1., 5., 5.]]])\n",
            "--------------------------------------------------------------------------------\n",
            "tensor([[[0.0000, 0.0000, 0.0303, 0.0417],\n",
            "         [0.0000, 0.0000, 0.0400, 0.0625],\n",
            "         [0.0000, 0.0000, 0.0000, 0.6400],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000]]])\n",
            "keep:  tensor([[ True,  True,  True, False]])\n",
            "********************************************************************************\n",
            "tensor([[4., 4., 7., 7.],\n",
            "        [1., 1., 2., 2.],\n",
            "        [0., 0., 5., 5.]]) tensor([0, 0, 0]) tensor([5.2300, 4.0000, 3.1200])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anxIuaghz0qE",
        "colab_type": "code",
        "outputId": "18593044-6381-4383-a5c0-1f4e3e5224d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "import torch\n",
        "a=torch.randn(2, 3)\n",
        "b=a.view(-1, 2)\n",
        "print(b)\n",
        "m=b.max()\n",
        "print(b.sum(1))\n",
        "print(torch.log(torch.sum(torch.exp(b-m), 1)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.2916,  1.1499],\n",
            "        [-0.8576, -0.3178],\n",
            "        [ 0.3299,  1.0939]])\n",
            "tensor([ 1.4416, -1.1754,  1.4239])\n",
            "tensor([ 0.3534, -1.0085,  0.3264])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wv98M-_NEjsJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ohem_conf_loss(self, class_p, conf_gt, positive_bool):\n",
        "        # class_P : [n, 19248, 81]\n",
        "        # Compute max conf across batch for hard negative mining\n",
        "        batch_conf = class_p.view(-1, self.num_classes)  # [n*19248, 81]\n",
        "\n",
        "        # compute softmax for each anchors\n",
        "        batch_conf_max = batch_conf.data.max()\n",
        "        mark = torch.log(torch.sum(torch.exp(batch_conf - batch_conf_max), 1)) + batch_conf_max - batch_conf[:, 0]     # [n*19248]\n",
        "\n",
        "        # Hard Negative Mining\n",
        "        mark = mark.view(class_p.size(0), -1)  # (n, 19248)\n",
        "        mark[positive_bool] = 0  # filter out pos boxes\n",
        "        mark[conf_gt < 0] = 0  # filter out neutrals (conf_gt = -1)\n",
        "\n",
        "        _, idx = mark.sort(1, descending=True)\n",
        "        _, idx_rank = idx.sort(1)\n",
        "\n",
        "        num_pos = positive_bool.long().sum(1, keepdim=True)\n",
        "        num_neg = torch.clamp(self.negpos_ratio * num_pos, max=positive_bool.size(1) - 1)\n",
        "\n",
        "        # Select num_neg hard negative examples.\n",
        "        negative_bool = idx_rank < num_neg.expand_as(idx_rank)\n",
        "\n",
        "        negative_bool[positive_bool] = 0    # filter out pos boxes\n",
        "        negative_bool[conf_gt < 0] = 0  # filter out neutrals\n",
        "\n",
        "        # Confidence Loss Including Positive and Negative Examples\n",
        "        class_p_selected = class_p[(positive_bool + negative_bool)].view(-1, self.num_classes)\n",
        "        class_gt_selected = conf_gt[(positive_bool + negative_bool)]\n",
        "\n",
        "        loss_c = F.cross_entropy(class_p_selected, class_gt_selected, reduction='sum')\n",
        "\n",
        "        return cfg.conf_alpha * loss_c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xD98m9fCTJpi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bbox_loss(pos_box_p, pos_offsets):\n",
        "        loss_b = F.smooth_l1_loss(pos_box_p, pos_offsets, reduction='sum') * cfg.bbox_alpha\n",
        "        return loss_b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqeYTjMPTPjY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lincomb_mask_loss(positive_bool, prior_max_index, coef_p, proto_p, mask_gt, prior_max_box):\n",
        "        proto_h = proto_p.size(1)  # 138\n",
        "        proto_w = proto_p.size(2)  # 138\n",
        "\n",
        "        loss_m = 0\n",
        "        for i in range(coef_p.size(0)):  # coef_p.shape: (n, 19248, 32)\n",
        "            with torch.no_grad():\n",
        "                # downsample the gt mask to the size of 'proto_p'\n",
        "                downsampled_masks = F.interpolate(mask_gt[i].unsqueeze(0), (proto_h, proto_w), mode='bilinear',\n",
        "                                                  align_corners=False).squeeze(0)\n",
        "                downsampled_masks = downsampled_masks.permute(1, 2, 0).contiguous()  # (138, 138, num_objects)\n",
        "                # binarize the gt mask because of the downsample operation\n",
        "                downsampled_masks = downsampled_masks.gt(0.5).float()\n",
        "\n",
        "            # gt boxes that corresponds to positive anchors (Not background)\n",
        "            # Not detected positive objects will not be computed for loss\n",
        "            pos_prior_index = prior_max_index[i, positive_bool[i]]  # pos_prior_index.shape: [num_positives]\n",
        "            pos_prior_box = prior_max_box[i, positive_bool[i]]\n",
        "            pos_coef = coef_p[i, positive_bool[i]]\n",
        "\n",
        "            if pos_prior_index.size(0) == 0:\n",
        "                continue\n",
        "\n",
        "            # If exceeds the number of masks for training, select a random subset\n",
        "            old_num_pos = pos_coef.size(0)\n",
        "            if old_num_pos > cfg.masks_to_train:\n",
        "                perm = torch.randperm(pos_coef.size(0))\n",
        "                select = perm[:cfg.masks_to_train]\n",
        "                pos_coef = pos_coef[select]\n",
        "                pos_prior_index = pos_prior_index[select]\n",
        "                pos_prior_box = pos_prior_box[select]\n",
        "\n",
        "            num_pos = pos_coef.size(0)\n",
        "\n",
        "            # Also choose positive objects' mask from downsampled_masks which is consists of all of objects' mask\n",
        "            pos_mask_gt = downsampled_masks[:, :, pos_prior_index]\n",
        "\n",
        "            # mask assembly by linear combination\n",
        "            # @ means dot product\n",
        "            # coef_p : [num_pos, 32] / proto_p : [138, 138, 32]\n",
        "            mask_p = torch.sigmoid(proto_p[i] @ pos_coef.t())  # mask_p.shape: (138, 138, num_pos)\n",
        "            mask_p = crop(mask_p, pos_prior_box)  # pos_prior_box.shape: (num_pos, 4)\n",
        "\n",
        "            mask_loss = F.binary_cross_entropy(torch.clamp(mask_p, 0, 1), pos_mask_gt, reduction='none')\n",
        "            # Normalize the mask loss to emulate roi pooling's effect on loss.\n",
        "            # Compute mask loss pixel-wise\n",
        "            pos_get_csize = center_size(pos_prior_box)      # Compute area of bounding box\n",
        "            mask_loss = mask_loss.sum(dim=(0, 1)) / pos_get_csize[:, 2] / pos_get_csize[:, 3]\n",
        "\n",
        "            if old_num_pos > num_pos:\n",
        "                mask_loss *= old_num_pos / num_pos\n",
        "\n",
        "            loss_m += torch.sum(mask_loss)\n",
        "\n",
        "        loss_m *= cfg.mask_alpha / proto_h / proto_w\n",
        "\n",
        "        return loss_m\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4z-SjbczbieJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    def semantic_segmentation_loss(segmentation_p, mask_gt, class_gt):\n",
        "        # Note classes here exclude the background class, so num_classes = cfg.num_classes-1\n",
        "        batch_size, num_classes, mask_h, mask_w = segmentation_p.size()  # (n, 80, 69, 69)\n",
        "        loss_s = 0\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            cur_segment = segmentation_p[i]\n",
        "            cur_class_gt = class_gt[i]\n",
        "\n",
        "            with torch.no_grad():\n",
        "                downsampled_masks = F.interpolate(mask_gt[i].unsqueeze(0), (mask_h, mask_w), mode='bilinear',\n",
        "                                                  align_corners=False).squeeze(0)\n",
        "                downsampled_masks = downsampled_masks.gt(0.5).float()  # (num_objects, 69, 69)\n",
        "\n",
        "                # Construct Semantic Segmentation\n",
        "                segment_gt = torch.zeros_like(cur_segment, requires_grad=False)\n",
        "                for i_obj in range(downsampled_masks.size(0)):\n",
        "                    segment_gt[cur_class_gt[i_obj]] = torch.max(segment_gt[cur_class_gt[i_obj]],\n",
        "                                                                downsampled_masks[i_obj])\n",
        "\n",
        "            loss_s += F.binary_cross_entropy_with_logits(cur_segment, segment_gt, reduction='sum')\n",
        "        # also compute loss pixel-wise\n",
        "        return loss_s / mask_h / mask_w * cfg.semantic_alpha"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}